from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
from datetime import datetime
import os
import re
import requests
import random
from urllib.parse import urlparse
from fake_useragent import UserAgent
from PIL import Image
import io
from webdriver_manager.chrome import ChromeDriverManager


# Configuration
# CHROMEDRIVER_PATH = "/usr/local/bin/chromedriver-136"
OUTPUT_FILE_BASE = "yupoo_data"

# Initialiser le gÃ©nÃ©rateur d'agent utilisateur
ua = UserAgent()

def clean_product_name(name):
    """Nettoyer le nom du produit - MAX 2 MOTS SEULEMENT"""
    if not name or name == "Name not found":
        return "PRODUIT"
    
    try:
        # Supprimer les numÃ©ros au dÃ©but (comme "200")
        name = re.sub(r'^\d+\s*', '', name)
        
        # Supprimer les caractÃ¨res chinois/japonais/corÃ©ens
        name = re.sub(r'[\u4e00-\u9fff\u3400-\u4dbf\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]+', '', name)
        
        # Supprimer les codes produit (comme HM6803-004, 03YHLS12)
        name = re.sub(r'[A-Z]{2,}\d{4,}-\d{3,}', '', name)
        name = re.sub(r'\d{2,}[A-Z]{2,}\d{2,}', '', name)
        name = re.sub(r'è´§å·[ï¼š:][A-Z0-9-]+', '', name)
        
        # Supprimer les tailles (comme 36-45, 36-46, Size 36 46)
        name = re.sub(r'Size\s*\d{2}\s*-?\s*\d{2}', '', name, flags=re.IGNORECASE)
        name = re.sub(r'\d{2}-\d{2}', '', name)
        
        # Supprimer les mots/codes inutiles
        useless_words = ['è´§å·', 'Size', 'Teal', 'Blue', '-', '_']
        for word in useless_words:
            name = re.sub(r'\b' + re.escape(word) + r'\b', '', name, flags=re.IGNORECASE)
        
        # Supprimer les caractÃ¨res spÃ©ciaux et espaces multiples
        name = re.sub(r'[^\w\s]', ' ', name)
        name = re.sub(r'\s+', ' ', name)
        name = name.strip()
        
        # Si le nom est vide aprÃ¨s nettoyage, utiliser un nom par dÃ©faut
        if not name:
            return "PRODUIT"
        
        # GARDER SEULEMENT LES 2 PREMIERS MOTS SIGNIFICATIFS
        words = [word.upper() for word in name.split() if len(word) > 1]
        
        # Filtrer les mots qui sont juste des numÃ©ros ou des codes
        meaningful_words = []
        for word in words:
            # Ignorer les mots qui sont principalement des numÃ©ros
            if not re.match(r'^\d+$', word) and not re.match(r'^V?\d+$', word):
                meaningful_words.append(word)
            # Mais garder les versions comme "700V2"
            elif re.match(r'^\d{3}V\d+$', word):
                meaningful_words.append(word)
        
        # Prendre les 2 premiers mots significatifs
        if len(meaningful_words) >= 2:
            result = f"{meaningful_words[0]}_{meaningful_words[1]}"
        elif len(meaningful_words) == 1:
            result = meaningful_words[0]
        else:
            result = "PRODUIT"
        
        # Limiter la longueur totale Ã  20 caractÃ¨res
        if len(result) > 20:
            result = result[:20]
        
        return result
        
    except Exception as e:
        print(f"   âš ï¸ Erreur lors du nettoyage du nom: {str(e)}")
        return "PRODUIT"

def create_session():
    """CrÃ©er une session avec protection anti-bot"""
    session = requests.Session()
    
    # En-tÃªtes amÃ©liorÃ©s pour contourner la protection anti-bot
    session.headers.update({
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'Referer': 'https://www.google.com/',
    })
    
    return session

def is_placeholder_image(response, url):
    """VÃ©rifier si l'image tÃ©lÃ©chargÃ©e est un placeholder"""
    
    # VÃ©rifier la taille du contenu (les images placeholder sont gÃ©nÃ©ralement trÃ¨s petites)
    content_length = len(response.content)
    if content_length < 1000:  # Moins de 1KB probablement un placeholder
        print(f"   âš ï¸ Image trop petite ({content_length} octets) - probablement un placeholder")
        return True
    
    # VÃ©rifier si redirigÃ© vers des URLs placeholder connues
    placeholder_indicators = [
        'placeholder', 'default', 'error', 'not-found', '404', 
        'forbidden', 'unavailable', 'res/703.gif', 'static/error'
    ]
    
    final_url = response.url.lower()
    for indicator in placeholder_indicators:
        if indicator in final_url:
            print(f"   âš ï¸ URL placeholder dÃ©tectÃ©e: {indicator}")
            return True
    
    # VÃ©rifier le type de contenu
    content_type = response.headers.get('Content-Type', '').lower()
    if content_type and not content_type.startswith('image/'):
        print(f"   âš ï¸ Type de contenu invalide: {content_type}")
        return True
    
    return False

def convert_to_webp(image_data, quality=80):
    """Convertir les donnÃ©es d'image en format WebP"""
    try:
        print(f"   ğŸ”„ Conversion WebP: {len(image_data)} octets d'entrÃ©e")
        
        # Ouvrir l'image depuis les donnÃ©es binaires
        img = Image.open(io.BytesIO(image_data))
        print(f"   ğŸ“ Dimensions originales: {img.size}, Mode: {img.mode}")
        
        # Si l'image a un canal alpha, la convertir en RGB
        if img.mode in ('RGBA', 'LA', 'P'):
            print(f"   ğŸ”„ Conversion de {img.mode} vers RGB")
            img = img.convert('RGB')
        
        # Sauvegarder en WebP dans un buffer
        webp_buffer = io.BytesIO()
        img.save(webp_buffer, format='WebP', quality=quality)
        webp_buffer.seek(0)
        
        webp_data = webp_buffer.getvalue()
        print(f"   âœ… Conversion rÃ©ussie: {len(webp_data)} octets de sortie")
        
        return webp_data
    except Exception as e:
        print(f"   âŒ ERREUR DE CONVERSION WebP: {str(e)}")
        return None

def download_image(url, output_dir, image_counter, session, folder_name):
    """TÃ©lÃ©charger une image avec protection anti-bot et conversion WebP"""
    
    try:
        # Nettoyer et valider l'URL
        url = str(url).strip()
        if not url.startswith(('http://', 'https://')):
            print(f"   âŒ ERREUR: Format d'URL invalide - {url}")
            return False, None, None
        
        # Ajouter un dÃ©lai alÃ©atoire pour Ã©viter la limitation de dÃ©bit
        time.sleep(random.uniform(0.5, 2.0))
        
        # Mettre Ã  jour le referrer pour chaque requÃªte
        if 'yupoo.com' in url:
            domain = urlparse(url).netloc
            session.headers.update({'Referer': f'https://{domain}/'})
        
        print(f"   ğŸ”„ TÃ©lÃ©chargement: {url[:60]}...")
        
        # PremiÃ¨re tentative
        response = session.get(url, timeout=30, allow_redirects=True)
        
        if response.status_code != 200:
            print(f"   âŒ ERREUR: Code de statut HTTP {response.status_code}")
            return False, None, None
        
        if is_placeholder_image(response, url):
            print(f"   âŒ ERREUR: Image placeholder dÃ©tectÃ©e")
            return False, None, None
        
        # Convertir en WebP
        print(f"   ğŸ”„ Conversion en WebP...")
        webp_data = convert_to_webp(response.content)
        
        if webp_data is None:
            print(f"   âŒ ERREUR: Ã‰chec de la conversion WebP")
            return False, None, None
        
        # GÃ©nÃ©rer le nom de fichier SIMPLE : img-1.webp, img-2.webp, etc.
        filename = f"img-{image_counter}.webp"
        file_path = os.path.join(output_dir, filename)
        
        # Sauvegarder le fichier WebP
        try:
            with open(file_path, 'wb') as f:
                f.write(webp_data)
            
            # VÃ‰RIFICATION CRITIQUE: S'assurer que le fichier existe rÃ©ellement
            if not os.path.exists(file_path):
                print(f"   âŒ ERREUR CRITIQUE: Fichier non crÃ©Ã© - {file_path}")
                return False, None, None
            
            # VÃ©rifier la taille du fichier sauvegardÃ©
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                print(f"   âŒ ERREUR CRITIQUE: Fichier vide - {file_path}")
                os.remove(file_path)  # Supprimer le fichier vide
                return False, None, None
            
            print(f"   âœ… Fichier sauvegardÃ©: {filename} ({file_size} octets)")
            
        except Exception as save_error:
            print(f"   âŒ ERREUR DE SAUVEGARDE: {str(save_error)}")
            return False, None, None
        
        # GÃ©nÃ©rer l'URL du serveur
        server_url = f"http://app.madeinchina-ebook.com/images/{folder_name}/{filename}"
        
        print(f"   âœ… Image tÃ©lÃ©chargÃ©e: {filename}")
        print(f"   ğŸ”— URL serveur: {server_url}")
        return True, filename, server_url
        
    except requests.exceptions.Timeout:
        print(f"   âŒ ERREUR: Timeout lors du tÃ©lÃ©chargement de {url}")
        return False, None, None
    except requests.exceptions.ConnectionError:
        print(f"   âŒ ERREUR: ProblÃ¨me de connexion pour {url}")
        return False, None, None
    except Exception as e:
        print(f"   âŒ ERREUR DE TÃ‰LÃ‰CHARGEMENT pour {url}: {str(e)}")
        return False, None, None

# Setup Chrome options for headless mode
def get_driver():
    options = Options()
    # options.binary_location = "/Applications/Brave Browser.app/Contents/MacOS/Brave Browser"
    
    # Activer le mode headless pour que vous puissiez utiliser votre ordinateur normalement
    options.add_argument("--headless")
    
    # Optimisations de performance
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-images")  # Ne pas charger les images pour un scraping plus rapide
    
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

def detect_pagination(driver, base_url):
    """DÃ©tecter si la page a une pagination et retourner le nombre total de pages"""
    print("ğŸ” VÃ©rification de la pagination...")
    
    try:
        driver.get(base_url)
        time.sleep(3)
        
        # Chercher les Ã©lÃ©ments de pagination
        pagination_selectors = [
            ".pagination__main",
            ".pagination",
            "[class*='pagination']"
        ]
        
        pagination_found = False
        total_pages = 1
        
        for selector in pagination_selectors:
            try:
                pagination_element = driver.find_element(By.CSS_SELECTOR, selector)
                pagination_found = True
                print("âœ… Pagination dÃ©tectÃ©e!")
                
                # Essayer de trouver le nombre total de pages de diffÃ©rentes maniÃ¨res
                page_methods = [
                    # MÃ©thode 1: Chercher le texte "au total X pages"
                    lambda: re.search(r'au total (\d+) pages?', pagination_element.text),
                    # MÃ©thode 2: Chercher le texte "total X pages"
                    lambda: re.search(r'total (\d+) pages?', pagination_element.text.lower()),
                    # MÃ©thode 3: Compter les liens de numÃ©ro de page
                    lambda: len(driver.find_elements(By.CSS_SELECTOR, ".pagination__number, .pagination-number, [class*='pagination'] a[href*='page=']")),
                    # MÃ©thode 4: Trouver le numÃ©ro de page le plus Ã©levÃ©
                    lambda: max([int(re.search(r'page=(\d+)', link.get_attribute('href')).group(1)) 
                               for link in driver.find_elements(By.CSS_SELECTOR, "a[href*='page=']") 
                               if re.search(r'page=(\d+)', link.get_attribute('href'))], default=1)
                ]
                
                for method in page_methods:
                    try:
                        result = method()
                        if isinstance(result, re.Match):
                            total_pages = int(result.group(1))
                            break
                        elif isinstance(result, int) and result > 0:
                            total_pages = result
                            break
                    except:
                        continue
                
                break
                
            except:
                continue
        
        if not pagination_found:
            print("â„¹ï¸  Aucune pagination trouvÃ©e - page unique dÃ©tectÃ©e")
            total_pages = 1
        else:
            print(f"ğŸ“„ Nombre total de pages dÃ©tectÃ©: {total_pages}")
        
        return total_pages, pagination_found
        
    except Exception as e:
        print(f"âš ï¸  Erreur lors de la dÃ©tection de pagination: {str(e)}")
        return 1, False

def create_output_folder():
    """CrÃ©er un dossier personnalisÃ© pour stocker les rÃ©sultats"""
    print("\nğŸ“ Configuration du dossier")
    print("Choisissez une option:")
    print("1. Entrer un nom de dossier personnalisÃ©")
    print("2. Utiliser un dossier avec horodatage (auto)")
    
    choice = input("Entrez votre choix (1 ou 2): ").strip()
    
    if choice == "1":
        folder_name = input("Entrez le nom du dossier: ").strip()
        if not folder_name:
            folder_name = f"scrape_yupoo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    else:
        folder_name = f"scrape_yupoo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # CrÃ©er le dossier principal
    try:
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
            print(f"âœ… Dossier crÃ©Ã©: {folder_name}")
        else:
            print(f"ğŸ“ Utilisation du dossier existant: {folder_name}")
        
        # CrÃ©er le sous-dossier images
        images_dir = os.path.join(folder_name, "images")
        if not os.path.exists(images_dir):
            os.makedirs(images_dir)
            print(f"âœ… Dossier images crÃ©Ã©: {images_dir}")
        
        return folder_name, images_dir
    except Exception as e:
        print(f"âŒ Erreur lors de la crÃ©ation du dossier: {str(e)}")
        print("ğŸ“ Utilisation du rÃ©pertoire actuel Ã  la place")
        images_dir = os.path.join(".", "images")
        if not os.path.exists(images_dir):
            os.makedirs(images_dir)
        return ".", images_dir

def get_base_url():
    """Obtenir l'URL de base de l'utilisateur"""
    print("\nğŸŒ Configuration de l'URL")
    print("Entrez l'URL de la catÃ©gorie Yupoo Ã  scraper:")
    print("Exemples:")
    print("- https://umkao.x.yupoo.com/categories/511015?isSubCate=true")
    print("- https://umkao.x.yupoo.com/categories/15850?isSubCate=true")
    
    url = input("\nEntrez l'URL: ").strip()
    
    # Nettoyer l'URL - supprimer le paramÃ¨tre page s'il est prÃ©sent
    if "page=" in url:
        url = re.sub(r'&page=\d+', '', url)
        url = re.sub(r'\?page=\d+&?', '?', url)
        url = re.sub(r'\?$', '', url)
    
    print(f"âœ… URL de base dÃ©finie: {url}")
    return url

def build_page_url(base_url, page_num, has_pagination):
    """Construire l'URL pour une page spÃ©cifique"""
    if not has_pagination or page_num == 1:
        return base_url
    
    # Ajouter le paramÃ¨tre page
    separator = "&" if "?" in base_url else "?"
    return f"{base_url}{separator}page={page_num}"

def scrape_page(driver, base_url, page_num, has_pagination, images_dir, session, folder_name, image_counter):
    """Scraper tous les produits d'une seule page"""
    page_url = build_page_url(base_url, page_num, has_pagination)
    print(f"\nğŸ” Scraping de la page {page_num}: {page_url}")
    
    try:
        driver.get(page_url)
        
        # Attendre le chargement des produits
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "a.album__main")))
        time.sleep(3)
        
        # Obtenir tous les liens produits de cette page
        items = driver.find_elements(By.CSS_SELECTOR, "a.album__main")
        print(f"ğŸ“¦ {len(items)} produits trouvÃ©s sur la page {page_num}")
        
        page_data = []
        
        for i, item in enumerate(items, 1):
            try:
                link = item.get_attribute('href')
                print(f"   Traitement de l'article {i}/{len(items)}: {link.split('/')[-1]}")
                
                # Ouvrir la page produit dans un nouvel onglet
                driver.execute_script("window.open(arguments[0]);", link)
                driver.switch_to.window(driver.window_handles[1])
                
                # Attendre le chargement de la page
                time.sleep(2)
                
                # Extraire les donnÃ©es du produit avec le compteur d'images
                product_data = extract_product_data(driver, link, page_num, images_dir, session, folder_name, image_counter)
                if product_data:
                    page_data.append(product_data)
                    image_counter[0] += 1  # IncrÃ©menter le compteur global
                
                # Fermer l'onglet et retourner Ã  la page principale
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                
            except Exception as e:
                print(f"   âŒ Erreur lors du traitement de l'article {i}: {str(e)}")
                # S'assurer qu'on retourne Ã  la fenÃªtre principale
                while len(driver.window_handles) > 1:
                    driver.close()
                    if len(driver.window_handles) > 0:
                        driver.switch_to.window(driver.window_handles[0])
                continue
        
        print(f"âœ… Page {page_num} terminÃ©e: {len(page_data)} articles scrapÃ©s")
        return page_data
        
    except Exception as e:
        print(f"âŒ Erreur lors du scraping de la page {page_num}: {str(e)}")
        return []

def extract_product_data(driver, link, page_num, images_dir, session, folder_name, image_counter):
    """Extraire les donnÃ©es d'une page produit individuelle"""
    try:
        # Utiliser le bon sÃ©lecteur CSS pour le nom du produit
        name = "Nom non trouvÃ©"
        
        # SÃ©lecteur principal - celui que vous avez spÃ©cifiÃ©
        try:
            element = driver.find_element(By.CSS_SELECTOR, ".showalbumheader__gallerytitle")
            if element.text.strip():
                name = element.text.strip()
        except:
            # SÃ©lecteurs de fallback si le principal Ã©choue
            title_selectors = [
                "span[data-name]",
                ".showalbumheader__title", 
                "h1", "h2"
            ]
            
            for selector in title_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    if element.text.strip():
                        name = element.text.strip()
                        break
                    # Essayer l'attribut data-name si le texte est vide
                    if selector == "span[data-name]":
                        data_name = element.get_attribute('data-name')
                        if data_name:
                            name = data_name
                            break
                except:
                    continue
        
        # Nettoyer le nom du produit (MAX 2 MOTS)
        clean_name = clean_product_name(name)
        
        print(f"   ğŸ“ Nom original: {name[:60]}...")
        print(f"   ğŸ·ï¸  Nom nettoyÃ©: {clean_name}")
        print(f"   ğŸ–¼ï¸  Nom image: img-{image_counter[0]}")
        
        # Extraire l'URL de l'image principale
        image_url = "Image non trouvÃ©e"
        image_selectors = [
            ".showalbumheader__gallerycover img",
            ".album-cover img",
            ".gallery img:first-child",
            "img[src*='yupoo.com']"
        ]
        
        for selector in image_selectors:
            try:
                img_element = driver.find_element(By.CSS_SELECTOR, selector)
                src = img_element.get_attribute('src')
                if src and 'yupoo.com' in src:
                    image_url = src
                    print(f"   ğŸ–¼ï¸  URL image trouvÃ©e: {src[:50]}...")
                    break
            except:
                continue
        
        # TÃ©lÃ©charger l'image si une URL valide est trouvÃ©e
        downloaded_image = None
        server_url = None
        download_status = "âŒ Ã‰CHEC"
        
        if image_url != "Image non trouvÃ©e":
            print(f"   ğŸ”„ TÃ©lÃ©chargement image #{image_counter[0]}...")
            success, filename, generated_server_url = download_image(image_url, images_dir, image_counter[0], session, folder_name)
            if success:
                downloaded_image = filename
                server_url = generated_server_url
                download_status = "âœ… RÃ‰USSI"
                print(f"   âœ… Image sauvÃ©e: {filename}")
            else:
                print(f"   âŒ Ã‰CHEC du tÃ©lÃ©chargement")
                download_status = "âŒ Ã‰CHEC - Voir logs dÃ©taillÃ©s"
        else:
            print(f"   âŒ ERREUR: Aucune URL d'image trouvÃ©e")
            download_status = "âŒ Ã‰CHEC - URL introuvable"
        
        # Retourner la structure de donnÃ©es
        return {
            'Nom_Produit': clean_name,
            'Nom_Original': name,
            'Lien_Article': link,
            'URL_Image_Originale': image_url,
            'URL_Image_Serveur': server_url if server_url else "Non disponible",
            'Image_Telecharge': downloaded_image if downloaded_image else "Non tÃ©lÃ©chargÃ©e",
            'Statut_Telechargement': download_status,
            'Numero_Page': page_num,
            'Date_Scraping': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
    except Exception as e:
        print(f"   âš ï¸  Erreur lors de l'extraction des donnÃ©es: {str(e)}")
        return None

def verify_downloaded_files(images_dir):
    """VÃ©rifier les fichiers rÃ©ellement tÃ©lÃ©chargÃ©s dans le dossier"""
    try:
        if not os.path.exists(images_dir):
            return 0, []
        
        files = os.listdir(images_dir)
        webp_files = [f for f in files if f.endswith('.webp')]
        
        print(f"\nğŸ” VÃ‰RIFICATION DU DOSSIER {images_dir}:")
        print(f"   ğŸ“ Fichiers WebP trouvÃ©s: {len(webp_files)}")
        
        if webp_files:
            print(f"   ğŸ“‹ Liste des fichiers:")
            for i, filename in enumerate(webp_files, 1):
                file_path = os.path.join(images_dir, filename)
                file_size = os.path.getsize(file_path)
                print(f"      {i}. {filename} ({file_size} octets)")
        
        return len(webp_files), webp_files
        
    except Exception as e:
        print(f"   âŒ Erreur lors de la vÃ©rification: {str(e)}")
        return 0, []

def save_to_files(all_data, base_filename, output_folder):
    """Sauvegarder les donnÃ©es scrapÃ©es dans des fichiers CSV et Excel"""
    if not all_data:
        print("âŒ Aucune donnÃ©e Ã  sauvegarder!")
        return
    
    df = pd.DataFrame(all_data)
    
    # Chemins des fichiers
    csv_filepath = os.path.join(output_folder, f"{base_filename}.csv")
    excel_filepath = os.path.join(output_folder, f"{base_filename}.xlsx")
    
    # CrÃ©er des sauvegardes si les fichiers existent
    for filepath in [csv_filepath, excel_filepath]:
        if os.path.exists(filepath):
            backup_name = f"{filepath.split('.')[0]}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{filepath.split('.')[-1]}"
            os.rename(filepath, backup_name)
            print(f"ğŸ“‹ Fichier existant sauvegardÃ© sous: {backup_name}")
    
    try:
        # Sauvegarder en CSV
        df.to_csv(csv_filepath, index=False, encoding='utf-8')
        print(f"ğŸ“„ CSV sauvegardÃ© dans: {csv_filepath}")
        
        # Sauvegarder en Excel avec formatage
        with pd.ExcelWriter(excel_filepath, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Produits_Yupoo', index=False)
            
            # Formater le fichier Excel
            workbook = writer.book
            worksheet = writer.sheets['Produits_Yupoo']
            
            # DÃ©finir les largeurs de colonnes
            column_widths = {
                'A': 25,  # Nom_Produit
                'B': 60,  # Nom_Original
                'C': 60,  # Lien_Article
                'D': 70,  # URL_Image_Originale
                'E': 70,  # URL_Image_Serveur
                'F': 20,  # Image_Telecharge
                'G': 20,  # Statut_Telechargement
                'H': 12,  # Numero_Page
                'I': 20   # Date_Scraping
            }
            
            for col, width in column_widths.items():
                worksheet.column_dimensions[col].width = width
            
            # Styliser les en-tÃªtes
            from openpyxl.styles import Font, PatternFill, Alignment
            
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            
            for cell in worksheet[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center")
        
        print(f"ğŸ“Š Excel sauvegardÃ© dans: {excel_filepath}")
        
        print(f"\nğŸ‰ SUCCÃˆS!")
        print(f"ğŸ“Š Total d'articles scrapÃ©s: {len(all_data)}")
        print(f"ğŸ“ˆ Pages scrapÃ©es: {df['Numero_Page'].nunique()}")
        
        # Afficher le rÃ©sumÃ© par page
        page_summary = df.groupby('Numero_Page').size()
        print(f"\nğŸ“‹ Articles par page:")
        for page, count in page_summary.items():
            print(f"   Page {page}: {count} articles")
        
        # Compter les images selon les statuts dans les donnÃ©es
        success_count = len(df[df['Statut_Telechargement'].str.contains('âœ…', na=False)])
        failed_count = len(df[df['Statut_Telechargement'].str.contains('âŒ', na=False)])
        
        print(f"\nğŸ–¼ï¸  RÃ‰SULTATS DE TÃ‰LÃ‰CHARGEMENT (selon les logs):")
        print(f"   âœ… Images dÃ©clarÃ©es rÃ©ussies: {success_count}/{len(all_data)}")
        print(f"   âŒ Images dÃ©clarÃ©es Ã©chouÃ©es: {failed_count}/{len(all_data)}")
        print(f"   ğŸ“Š Taux de rÃ©ussite dÃ©clarÃ©: {(success_count/len(all_data)*100):.1f}%")
        
        # VÃ‰RIFICATION RÃ‰ELLE DES FICHIERS
        images_dir = os.path.join(output_folder, "images")
        actual_count, actual_files = verify_downloaded_files(images_dir)
        
        print(f"\nğŸ” VÃ‰RIFICATION RÃ‰ELLE DES FICHIERS:")
        print(f"   ğŸ“ Fichiers rÃ©ellement prÃ©sents: {actual_count}/{len(all_data)}")
        print(f"   ğŸ“Š Taux de rÃ©ussite RÃ‰EL: {(actual_count/len(all_data)*100):.1f}%")
        
        # Alerte si il y a une diffÃ©rence
        if actual_count != success_count:
            print(f"\nâš ï¸  ATTENTION: DIFFÃ‰RENCE DÃ‰TECTÃ‰E!")
            print(f"   ğŸ“Š DÃ©clarÃ©s rÃ©ussis: {success_count}")
            print(f"   ğŸ“ RÃ©ellement prÃ©sents: {actual_count}")
            print(f"   ğŸ” DiffÃ©rence: {success_count - actual_count} fichiers manquants")
            
            if actual_count < success_count:
                print(f"\nğŸ”§ CAUSES POSSIBLES:")
                print(f"   â€¢ Erreurs de sauvegarde de fichiers")
                print(f"   â€¢ ProblÃ¨mes de permissions")
                print(f"   â€¢ Conversions WebP Ã©chouÃ©es")
                print(f"   â€¢ Fichiers corrompus supprimÃ©s")
        
        if failed_count > 0 or actual_count < len(all_data):
            missing_count = len(all_data) - actual_count
            print(f"\nâš ï¸  ATTENTION: {missing_count} images manquantes!")
            print(f"   VÃ©rifiez la colonne 'Statut_Telechargement' dans le fichier Excel pour plus de dÃ©tails.")
            
    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde des fichiers: {str(e)}")

def main():
    """Fonction principale de scraping"""
    print("ğŸš€ Scraper Intelligent Yupoo (Images: img-1, img-2... | Noms: MAX 2 MOTS)")
    print("=" * 80)
    
    # VÃ©rifier si Pillow est installÃ©
    try:
        from PIL import Image
    except ImportError:
        print("âŒ ERREUR: Pillow n'est pas installÃ©!")
        print("ğŸ“¦ Installez-le avec: pip install Pillow")
        return
    
    # Obtenir l'URL et configurer le dossier
    base_url = get_base_url()
    output_folder, images_dir = create_output_folder()
    
    print(f"\nğŸ’¾ Dossier de sortie: {output_folder}")
    print(f"ğŸ–¼ï¸  Dossier des images: {images_dir}")
    print("ğŸ’» Vous pouvez continuer Ã  utiliser votre ordinateur normalement pendant le scraping!")
    
    start_time = datetime.now()
    all_scraped_data = []
    
    # COMPTEUR GLOBAL POUR LES NOMS D'IMAGES (img-1, img-2, etc.)
    image_counter = [1]  # Utilise une liste pour pouvoir la modifier dans les fonctions
    
    print("\nğŸ”‡ DÃ©marrage du navigateur en mode headless...")
    driver = get_driver()
    
    # CrÃ©er une session pour le tÃ©lÃ©chargement d'images
    print("ğŸ›¡ï¸ Initialisation de la session de tÃ©lÃ©chargement avec protection anti-bot...")
    session = create_session()
    
    try:
        # DÃ©tecter la pagination
        total_pages, has_pagination = detect_pagination(driver, base_url)
        
        if has_pagination:
            print(f"ğŸ“„ Va scraper {total_pages} pages")
        else:
            print("ğŸ“„ Va scraper 1 page (pas de pagination)")
        
        print(f"\nğŸ·ï¸  SYSTÃˆME DE NOMMAGE:")
        print(f"   ğŸ“ Images: img-1.webp, img-2.webp, img-3.webp...")
        print(f"   ğŸ“ Produits: MAX 2 mots (ex: YEEZY_700V2)")
        
        # Scraper chaque page
        for page in range(1, total_pages + 1):
            page_data = scrape_page(driver, base_url, page, has_pagination, images_dir, session, os.path.basename(output_folder), image_counter)
            all_scraped_data.extend(page_data)
            
            # Sauvegarder le progrÃ¨s pÃ©riodiquement (toutes les 3 pages pour multi-page, ou aprÃ¨s page unique)
            if (has_pagination and page % 3 == 0) or (not has_pagination and page == 1):
                temp_csv = os.path.join(output_folder, f"temp_{OUTPUT_FILE_BASE}.csv")
                temp_excel = os.path.join(output_folder, f"temp_{OUTPUT_FILE_BASE}.xlsx")
                
                if all_scraped_data:
                    temp_df = pd.DataFrame(all_scraped_data)
                    temp_df.to_csv(temp_csv, index=False)
                    temp_df.to_excel(temp_excel, index=False)
                    print(f"ğŸ’¾ ProgrÃ¨s sauvegardÃ© (img-{image_counter[0]-1} images traitÃ©es)")
            
            # BrÃ¨ve pause entre les pages (seulement s'il y a plus de pages Ã  traiter)
            if page < total_pages:
                time.sleep(2)
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Scraping interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nâŒ Erreur inattendue: {str(e)}")
    finally:
        driver.quit()
        print("ğŸ”‡ Navigateur fermÃ©")
    
    # Sauvegarder les rÃ©sultats finaux
    if all_scraped_data:
        save_to_files(all_scraped_data, OUTPUT_FILE_BASE, output_folder)
        
        end_time = datetime.now()
        duration = end_time - start_time
        print(f"\nâ±ï¸  Temps total: {duration}")
        print(f"âš¡ Temps moyen par article: {duration.total_seconds() / len(all_scraped_data):.2f} secondes")
        print(f"ğŸ–¼ï¸  Dernier numÃ©ro d'image: img-{image_counter[0]-1}")
        
        # Nettoyer les fichiers temporaires
        for temp_file in [f"temp_{OUTPUT_FILE_BASE}.csv", f"temp_{OUTPUT_FILE_BASE}.xlsx"]:
            temp_path = os.path.join(output_folder, temp_file)
            if os.path.exists(temp_path):
                os.remove(temp_path)
                print(f"ğŸ—‘ï¸  NettoyÃ© {temp_file}")
        
        print(f"\nğŸ“ Tous les fichiers sauvegardÃ©s dans: {output_folder}")
        print(f"ğŸ–¼ï¸  Images WebP sauvegardÃ©es dans: {images_dir}")
        print(f"ğŸ“‹ Format des images: img-1.webp Ã  img-{image_counter[0]-1}.webp")
    else:
        print("\nâŒ Aucune donnÃ©e n'a Ã©tÃ© scrapÃ©e!")

if __name__ == "__main__":
    main()